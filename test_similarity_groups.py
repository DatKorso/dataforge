"""–¢–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑–º–µ—Ä–∞ –≥—Ä—É–ø–ø –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–µ similarity matching"""
from dataforge.similarity_matching import search_similar_matches
from dataforge.similarity_config import SimilarityScoringConfig
import os

# –ß–∏—Ç–∞–µ–º wb_sku –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
with open("docs/wb_sku_for_test.txt") as f:
    wb_skus = [line.strip() for line in f if line.strip()]

print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(wb_skus)} wb_sku –∏–∑ —Ñ–∞–π–ª–∞")
print(f"–ü–µ—Ä–≤—ã–µ 5: {wb_skus[:5]}")

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å –Ω–æ–≤—ã–º–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
cfg = SimilarityScoringConfig(
    min_score_threshold=300.0,
    max_candidates_per_seed=30,
    max_group_size=10,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã
)

# –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ streamlit secrets
md_token = os.environ.get("MOTHERDUCK_TOKEN")
md_database = os.environ.get("MOTHERDUCK_DATABASE", "dataforge")

if not md_token:
    try:
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ secrets —á–µ—Ä–µ–∑ dataforge.secrets
        from dataforge.secrets import load_secrets
        secrets = load_secrets()
        md_token = secrets.get("md_token")
        md_database = secrets.get("md_database", "dataforge")
        if md_token:
            print("‚úÖ –¢–æ–∫–µ–Ω—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ .streamlit/secrets.toml")
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ secrets: {e}")
        
if not md_token:
    print("‚ö†Ô∏è  MOTHERDUCK_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: export MOTHERDUCK_TOKEN=...")
    print("   –ò–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ .streamlit/secrets.toml")
    exit(1)

print("\nüîç –ó–∞–ø—É—Å–∫–∞–µ–º search_similar_matches —Å –Ω–æ–≤—ã–º–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
df_result = search_similar_matches(
    wb_skus,
    config=cfg,
    md_token=md_token,
    md_database=md_database,
)

if df_result.empty:
    print("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—É—Å—Ç—ã–µ")
    exit(0)

print(f"\nüìä –ü–æ–ª—É—á–µ–Ω–æ {len(df_result)} —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö wb_sku: {df_result['wb_sku'].nunique()}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö oz_sku: {df_result['oz_sku'].nunique()}")

# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä—É–ø–ø—ã
if 'group_number' in df_result.columns:
    group_sizes = df_result.groupby('group_number')['wb_sku'].nunique()
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º:")
    print(f"–í—Å–µ–≥–æ –≥—Ä—É–ø–ø: {len(group_sizes)}")
    print(f"–†–∞–∑–º–µ—Ä—ã –≥—Ä—É–ø–ø (—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö wb_sku):")
    print(group_sizes.sort_values(ascending=False).head(10))
    
    max_group = group_sizes.max()
    max_group_num = group_sizes.idxmax()
    print(f"\n‚ö†Ô∏è  –°–∞–º–∞—è –±–æ–ª—å—à–∞—è –≥—Ä—É–ø–ø–∞: #{max_group_num} —Å {max_group} —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ wb_sku")
    
    if cfg.max_group_size and max_group > cfg.max_group_size:
        print(f"‚ùå –ü–†–û–ë–õ–ï–ú–ê: –†–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã {max_group} –ø—Ä–µ–≤—ã—à–∞–µ—Ç max_group_size={cfg.max_group_size}")
        print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: –Ω–µ –±–æ–ª–µ–µ {cfg.max_group_size}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º wb_sku –∏–∑ —Å–∞–º–æ–π –±–æ–ª—å—à–æ–π –≥—Ä—É–ø–ø—ã
        big_group = df_result[df_result['group_number'] == max_group_num]
        print(f"\n   WB SKU –≤ –±–æ–ª—å—à–æ–π –≥—Ä—É–ø–ø–µ (–ø–µ—Ä–≤—ã–µ 20):")
        unique_wb = big_group['wb_sku'].unique()[:20]
        for wb in unique_wb:
            print(f"     - {wb}")
    else:
        print(f"‚úÖ –†–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ–∂–∏–¥–∞–µ–º–æ–≥–æ (max_group_size={cfg.max_group_size})")
else:
    print("‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ group_number –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
df_result.to_csv("test_similarity_groups_output.csv", index=False)
print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ test_similarity_groups_output.csv")

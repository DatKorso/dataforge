from __future__ import annotations

import io
from dataclasses import dataclass, field

import pandas as pd
import streamlit as st
from dataforge.matching import search_matches
from dataforge.matching_helpers import add_merge_fields
from dataforge.ui import setup_page
import math

setup_page(title="DataForge ‚Äî –°–∫–ª–µ–π–∫–∞ OZ", icon="üß©")

DEFECT_PREFIX = "–ë—Ä–∞–∫SH"


@dataclass
class MergeConfig:
    md_token: str | None = None
    md_database: str | None = None
    filter_no_defect: bool = True
    filter_unique_sizes: bool = True
    limit_per_input: int = 0
    selected_cols: list[str] = field(default_factory=lambda: [
        "wb_sku",
        "oz_sku",
        "oz_vendor_code",
        "merge_code",
        "merge_color",
    ])


def parse_input(text: str) -> list[str]:
    tokens = [t.strip() for t in text.replace(",", " ").split()]
    return [t for t in tokens if t]


st.title("üß© –°–∫–ª–µ–π–∫–∞ –∫–∞—Ä—Ç–æ—á–µ–∫ OZ")
st.caption("–°–≤—è–∑—ã–≤–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ OZ –º–µ–∂–¥—É —Å–æ–±–æ–π –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É (–±–∞–∑–æ–≤—ã–π: –æ–±—â–∏–π wb_sku).")


md_token = st.session_state.get("md_token") or st.secrets.get("md_token") if hasattr(st, "secrets") else None
md_database = st.session_state.get("md_database") or st.secrets.get("md_database") if hasattr(st, "secrets") else None

if not md_token:
    st.warning("MD —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–∫–∞–∂–∏—Ç–µ –µ–≥–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ù–∞—Å—Ç—Ä–æ–π–∫–∏.")


with st.form(key="oz_merge_form"):
    col1, col2 = st.columns([1, 1])
    with col1:
        merge_algo = st.selectbox(
            "–ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è",
            options=[("–û–±—ä–µ–¥–∏–Ω–∏—Ç—å –ø–æ –æ–±—â–µ–º—É WB –∞—Ä—Ç–∏–∫—É–ª—É", "wb_by_sku")],
            format_func=lambda x: x[0] if isinstance(x, tuple) else x,
            key="oz_merge_algo",
        )

    with col2:
        st.number_input(
            "–õ–∏–º–∏—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (0 ‚Äî –±–µ–∑ –ª–∏–º–∏—Ç–∞)",
            min_value=0,
            max_value=10000,
            value=int(st.session_state.get("oz_merge_limit", 20)),
            key="oz_merge_limit",
        )

    st.text_area(
        "–°–ø–∏—Å–æ–∫ WB SKU",
        value=st.session_state.get("oz_merge_input_text", ""),
        height=140,
        help="–í—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ wb_sku —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –∏–ª–∏ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É.",
        key="oz_merge_input_text",
    )

    st.checkbox("–ë–µ–∑ –±—Ä–∞–∫–∞ –û–∑–æ–Ω", key="oz_merge_filter_no_defect", value=True, help="–ò—Å–∫–ª—é—á–∏—Ç—å oz_vendor_code –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è –Ω–∞ –ë—Ä–∞–∫SH")
    st.checkbox("–ë–µ–∑ –¥—É–±–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–≤", key="oz_merge_filter_unique_sizes", value=True, help="–û—Å—Ç–∞–≤–ª—è—Ç—å –ø–æ –æ–¥–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é —Ä–∞–∑–º–µ—Ä–∞ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º match_score")
    st.write("---")
    st.markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (chunking)**")
    # calibrated defaults from autotune
    st.number_input("–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (batch_size)", min_value=1, max_value=5000, value=int(st.session_state.get("oz_merge_batch_size", 1000)), key="oz_merge_batch_size")
    st.write(f"–õ–∏–º–∏—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –≤—Ö–æ–¥ (limit_per_input): {int(st.session_state.get('oz_merge_limit', 20))}")
    st.info("Batch processing –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø–æ–∏—Å–∫ –ø–∞—Ä—Ç–∏—è–º–∏ –∏ –æ–±–Ω–æ–≤–ª—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.")

    submitted = st.form_submit_button("–°–∫–ª–µ–∏—Ç—å", type="primary")

if submitted:
    if not md_token:
        st.error("MD —Ç–æ–∫–µ–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –£–∫–∞–∂–∏—Ç–µ –µ–≥–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ù–∞—Å—Ç—Ä–æ–π–∫–∏.")
        st.stop()

    text_value = st.session_state.get("oz_merge_input_text", "")
    values = parse_input(text_value)
    if not values:
        st.warning("–í–≤–µ–¥–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω WB SKU.")
        st.stop()

    if len(values) > 500:
        st.info("–ü–µ—Ä–µ–¥–∞–Ω–æ –º–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–π; –æ–ø–µ—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è.")

    try:
        # Batch processing with progress
        batch_size = int(st.session_state.get("oz_merge_batch_size", 1000))
        limit_per_input = int(st.session_state.get("oz_merge_limit", 20))
        total = len(values)
        n_chunks = math.ceil(total / batch_size)

        st.session_state["oz_merge_result"] = pd.DataFrame()
        progress = st.progress(0)
        fetched = 0

        for idx in range(n_chunks):
            start = idx * batch_size
            end = min(start + batch_size, total)
            chunk = values[start:end]
            with st.spinner(f"–ó–∞–ø—Ä–æ—Å –ø–∞—Ä—Ç–∏–∏ {idx+1}/{n_chunks} ({start+1}-{end})..."):
                df_chunk = search_matches(
                    chunk,
                    input_type="wb_sku",
                    limit_per_input=(None if limit_per_input <= 0 else limit_per_input),
                    md_token=md_token,
                    md_database=md_database,
                )

            if df_chunk is None or df_chunk.empty:
                df_chunk = pd.DataFrame()

            # Apply defect filter per-chunk to reduce memory
            if st.session_state.get("oz_merge_filter_no_defect", True) and "oz_vendor_code" in df_chunk.columns:
                starts_with_brak = df_chunk["oz_vendor_code"].fillna("").astype(str).str.startswith(DEFECT_PREFIX)
                df_chunk = df_chunk.loc[~starts_with_brak]

            # Add merge fields to the chunk
            if not df_chunk.empty and "oz_vendor_code" in df_chunk.columns:
                df_chunk = add_merge_fields(df_chunk, wb_sku_col="wb_sku", oz_vendor_col="oz_vendor_code")

            # accumulate safely
            df_result = st.session_state.get("oz_merge_result")
            if df_result is None or (hasattr(df_result, "empty") and df_result.empty):
                st.session_state["oz_merge_result"] = df_chunk.copy()
            else:
                st.session_state["oz_merge_result"] = pd.concat([df_result, df_chunk], ignore_index=True, copy=False)

            fetched += len(chunk)
            # Update progress
            progress.progress(int((end / total) * 100))

        # Final dedupe sizes similar to existing page
        if st.session_state.get("oz_merge_filter_unique_sizes", True) and not st.session_state["oz_merge_result"].empty:
            from dataforge.matching_helpers import dedupe_sizes

            st.session_state["oz_merge_result"] = dedupe_sizes(st.session_state["oz_merge_result"], input_type="wb_sku")

        # Ensure merge fields present (in case dedupe removed them)
        if not st.session_state["oz_merge_result"].empty:
            st.session_state["oz_merge_result"] = add_merge_fields(st.session_state["oz_merge_result"], wb_sku_col="wb_sku", oz_vendor_col="oz_vendor_code")

        # collect invalid oz_vendor_code rows for highlighting
        df = st.session_state["oz_merge_result"]
        if "oz_vendor_code" in df.columns:
            invalid_mask = ~df["oz_vendor_code"].astype(str).str.contains("-", regex=False)
            st.session_state["oz_merge_invalid_oz_vendor"] = df.loc[invalid_mask]
        else:
            st.session_state["oz_merge_invalid_oz_vendor"] = pd.DataFrame()
    except Exception as exc:
        st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ/–æ–±—Ä–∞–±–æ—Ç–∫–µ: ")
        st.exception(exc)

df_show: pd.DataFrame = st.session_state.get("oz_merge_result", pd.DataFrame())

if df_show.empty:
    st.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –ø–æ–∫–∞–∑–∞–Ω—ã –∑–¥–µ—Å—å –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è '–°–∫–ª–µ–∏—Ç—å'.")
    st.stop()

cols_default = [
    "wb_sku",
    "oz_sku",
    "oz_vendor_code",
    "merge_code",
    "merge_color",
    "match_score",
]

cols_to_show = [c for c in st.session_state.get("oz_merge_selected_cols", cols_default) if c in df_show.columns]
if not cols_to_show:
    cols_to_show = [c for c in cols_default if c in df_show.columns]

st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–ª–µ–π–∫–∏ OZ")
st.dataframe(df_show[cols_to_show], width="stretch", height=600)

csv_buf = io.StringIO()
df_show[cols_to_show].to_csv(csv_buf, index=False)
st.download_button(
    "–°–∫–∞—á–∞—Ç—å CSV",
    data=csv_buf.getvalue().encode("utf-8"),
    file_name="oz_merge.csv",
    mime="text/csv",
)

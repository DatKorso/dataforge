from __future__ import annotations

import io
import math
from dataclasses import dataclass, field

import pandas as pd
import streamlit as st
from dataforge.matching import search_matches
from dataforge.matching_helpers import add_merge_fields
from dataforge.similarity_matching import search_similar_matches  # –Ω–æ–≤—ã–π –ø—Ä—è–º–æ–π –∏–º–ø–æ—Ä—Ç
from dataforge.ui import setup_page

setup_page(title="DataForge ‚Äî –°–∫–ª–µ–π–∫–∞ OZ", icon="üß©")

DEFECT_PREFIX = "–ë—Ä–∞–∫SH"


@dataclass
class MergeConfig:
    md_token: str | None = None
    md_database: str | None = None
    filter_no_defect: bool = True
    filter_unique_sizes: bool = True
    limit_per_input: int = 0
    selected_cols: list[str] = field(default_factory=lambda: [
        "group_number",
        "wb_sku",
        "oz_sku",
        "oz_vendor_code",
        "merge_code",
        "merge_color",
        "similarity_score",
    ])


def parse_input(text: str) -> list[str]:
    tokens = [t.strip() for t in text.replace(",", " ").split()]
    return [t for t in tokens if t]


st.title("üß© –°–∫–ª–µ–π–∫–∞ –∫–∞—Ä—Ç–æ—á–µ–∫ OZ")
st.caption("–°–≤—è–∑—ã–≤–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ OZ –º–µ–∂–¥—É —Å–æ–±–æ–π –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É (–±–∞–∑–æ–≤—ã–π: –æ–±—â–∏–π wb_sku).")


md_token = st.session_state.get("md_token") or st.secrets.get("md_token") if hasattr(st, "secrets") else None
md_database = st.session_state.get("md_database") or st.secrets.get("md_database") if hasattr(st, "secrets") else None

if not md_token:
    st.warning("MD —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–∫–∞–∂–∏—Ç–µ –µ–≥–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ù–∞—Å—Ç—Ä–æ–π–∫–∏.")


with st.form(key="oz_merge_form"):
    col1, col2 = st.columns([1, 1])
    with col1:
        merge_algo = st.selectbox(
            "–ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è",
            options=[
                ("–û–±—ä–µ–¥–∏–Ω–∏—Ç—å –ø–æ –æ–±—â–µ–º—É WB –∞—Ä—Ç–∏–∫—É–ª—É", "wb_by_sku"),
                ("–û–±—ä–µ–¥–∏–Ω–∏—Ç—å –ø–æ –ø–æ—Ö–æ–∂–∏–º WB —Ç–æ–≤–∞—Ä–∞–º", "wb_similarity"),
            ],
            format_func=lambda x: x[0] if isinstance(x, tuple) else x,
            key="oz_merge_algo",
        )

    with col2:
        st.number_input(
            "–õ–∏–º–∏—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (0 ‚Äî –±–µ–∑ –ª–∏–º–∏—Ç–∞)",
            min_value=0,
            max_value=10000,
            value=int(st.session_state.get("oz_merge_limit", 20)),
            key="oz_merge_limit",
        )

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è similarity
    with st.expander("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (wb_similarity)"):
        st.number_input(
            "min_score_threshold",
            min_value=0.0,
            max_value=1000.0,
            value=float(st.session_state.get("oz_merge_min_score", 300.0)),
            key="oz_merge_min_score",
            help="–ü–æ—Ä–æ–≥ –æ—Ç—Å–µ—á–µ–Ω–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É —Å–∫–æ—Ä—É",
        )
        st.number_input(
            "max_candidates_per_seed",
            min_value=1,
            max_value=100,
            value=int(st.session_state.get("oz_merge_max_rec", 30)),
            key="oz_merge_max_rec",
            help="–ú–∞–∫—Å–∏–º—É–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã–π WB SKU",
        )
        st.number_input(
            "max_group_size",
            min_value=0,
            max_value=500,
            value=int(st.session_state.get("oz_merge_max_group_size", 10)),
            key="oz_merge_max_group_size",
            help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã (0 = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è). –ë–æ–ª—å—à–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –±—É–¥—É—Ç —Ä–∞–∑–±–∏—Ç—ã –Ω–∞ –ø–æ–¥–≥—Ä—É–ø–ø—ã.",
        )

    st.text_area(
        "–°–ø–∏—Å–æ–∫ WB SKU",
        value=st.session_state.get("oz_merge_input_text", ""),
        height=140,
        help="–í—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ wb_sku —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –∏–ª–∏ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É.",
        key="oz_merge_input_text",
    )

    st.checkbox("–ë–µ–∑ –±—Ä–∞–∫–∞ –û–∑–æ–Ω", key="oz_merge_filter_no_defect", value=True, help="–ò—Å–∫–ª—é—á–∏—Ç—å oz_vendor_code –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è –Ω–∞ –ë—Ä–∞–∫SH")
    st.checkbox("–ë–µ–∑ –¥—É–±–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–≤", key="oz_merge_filter_unique_sizes", value=True, help="–û—Å—Ç–∞–≤–ª—è—Ç—å –ø–æ –æ–¥–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é —Ä–∞–∑–º–µ—Ä–∞ —Å –Ω–∞–∏–≤—ã—Å—à–∏–º match_score")
    st.caption("‚ö†Ô∏è –ü–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –Ω–∞–∂–º–∏—Ç–µ '–°–∫–ª–µ–∏—Ç—å' —Å–Ω–æ–≤–∞ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π")
    st.write("---")
    st.markdown("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (chunking)**")
    # calibrated defaults from autotune
    st.number_input("–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (batch_size)", min_value=1, max_value=5000, value=int(st.session_state.get("oz_merge_batch_size", 1000)), key="oz_merge_batch_size")
    st.write(f"–õ–∏–º–∏—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –≤—Ö–æ–¥ (limit_per_input): {int(st.session_state.get('oz_merge_limit', 20))}")
    st.info("Batch processing –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø–æ–∏—Å–∫ –ø–∞—Ä—Ç–∏—è–º–∏ –∏ –æ–±–Ω–æ–≤–ª—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.")

    submitted = st.form_submit_button("–°–∫–ª–µ–∏—Ç—å", type="primary")

if submitted:
    if not md_token:
        st.error("MD —Ç–æ–∫–µ–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –£–∫–∞–∂–∏—Ç–µ –µ–≥–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ù–∞—Å—Ç—Ä–æ–π–∫–∏.")
        st.stop()

    text_value = st.session_state.get("oz_merge_input_text", "")
    values = parse_input(text_value)
    if not values:
        st.warning("–í–≤–µ–¥–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω WB SKU.")
        st.stop()

    if len(values) > 500:
        st.info("–ü–µ—Ä–µ–¥–∞–Ω–æ –º–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–π; –æ–ø–µ—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è.")

    try:
        if merge_algo[1] == "wb_similarity":
            from dataforge.similarity_config import SimilarityScoringConfig
            max_group_size_val = int(st.session_state.get("oz_merge_max_group_size", 15))
            cfg = SimilarityScoringConfig(
                min_score_threshold=float(st.session_state.get("oz_merge_min_score", 50.0)),
                max_candidates_per_seed=int(st.session_state.get("oz_merge_max_rec", 10)),
                max_group_size=max_group_size_val if max_group_size_val > 0 else None,
            )
            
            # Batch processing with progress
            batch_size = int(st.session_state.get("oz_merge_batch_size", 1000))
            total = len(values)
            n_chunks = math.ceil(total / batch_size)
            
            st.session_state["oz_merge_result"] = pd.DataFrame()
            progress = st.progress(0)
            fetched = 0
            
            for idx in range(n_chunks):
                start = idx * batch_size
                end = min(start + batch_size, total)
                chunk = values[start:end]
                with st.spinner(f"–ó–∞–ø—Ä–æ—Å –ø–∞—Ä—Ç–∏–∏ {idx+1}/{n_chunks} ({start+1}-{end})..."):
                    df_chunk = search_similar_matches(
                        chunk,
                        config=cfg,
                        md_token=md_token,
                        md_database=md_database,
                    )
                
                if df_chunk is None or df_chunk.empty:
                    df_chunk = pd.DataFrame()
                
                # Apply defect filter per-chunk to reduce memory
                if st.session_state.get("oz_merge_filter_no_defect", True) and "oz_vendor_code" in df_chunk.columns:
                    starts_with_brak = df_chunk["oz_vendor_code"].fillna("").astype(str).str.startswith(DEFECT_PREFIX)
                    df_chunk = df_chunk.loc[~starts_with_brak]
                
                # Accumulate safely
                df_result = st.session_state.get("oz_merge_result")
                if df_result is None or (hasattr(df_result, "empty") and df_result.empty):
                    st.session_state["oz_merge_result"] = df_chunk.copy()
                else:
                    st.session_state["oz_merge_result"] = pd.concat([df_result, df_chunk], ignore_index=True, copy=False)
                
                fetched += len(chunk)
                # Update progress
                progress.progress(int((end / total) * 100))
            
            # Handle duplicate sizes
            df_similarity = st.session_state["oz_merge_result"]
            
            if st.session_state.get("oz_merge_filter_unique_sizes", True):
                # Remove duplicates completely - filter by wb_sku + size (not group_number)
                if not df_similarity.empty and 'oz_manufacturer_size' in df_similarity.columns and 'wb_sku' in df_similarity.columns:
                    mask_known_size = df_similarity['oz_manufacturer_size'].notna() & (df_similarity['oz_manufacturer_size'].astype(str).str.strip() != "")
                    df_known = df_similarity.loc[mask_known_size].copy()
                    df_unknown = df_similarity.loc[~mask_known_size].copy()
                    
                    if not df_known.empty:
                        df_known = df_known.sort_values(['match_score'], ascending=[False])
                        # Dedupe by wb_sku + size (not group_number + size)
                        df_known = df_known.drop_duplicates(subset=['wb_sku', 'oz_manufacturer_size'], keep='first')
                        df_similarity = pd.concat([df_known, df_unknown], axis=0, ignore_index=True)
                    
                    st.session_state["oz_merge_result"] = df_similarity
            # Note: If filter_unique_sizes is False, duplicates are already handled by add_merge_fields_for_similarity
            
            # Check for missing wb_sku
            df_similarity = st.session_state["oz_merge_result"]
            found_wb = {str(x) for x in df_similarity.get("wb_sku", [])} if not df_similarity.empty else set()
            missing = [v for v in values if v not in found_wb]
            st.session_state["oz_merge_missing_wb"] = missing
            
            # Fallback: –∑–∞–ø—É—Å–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –Ω–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
            if missing:
                st.info(f"üìã –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã –¥–ª—è {len(missing)} wb_sku. –ó–∞–ø—É—Å–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –≥—Ä—É–ø–ø...")
                try:
                    with st.spinner(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(missing)} —Ç–æ–≤–∞—Ä–æ–≤ –±–∞–∑–æ–≤—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º..."):
                        df_fallback = search_matches(
                            missing,
                            input_type="wb_sku",
                            limit_per_input=None,
                            md_token=md_token,
                            md_database=md_database,
                        )
                        
                        if not df_fallback.empty:
                            # Apply defect filter
                            if st.session_state.get("oz_merge_filter_no_defect", True) and "oz_vendor_code" in df_fallback.columns:
                                starts_with_brak = df_fallback["oz_vendor_code"].fillna("").astype(str).str.startswith(DEFECT_PREFIX)
                                df_fallback = df_fallback.loc[~starts_with_brak]
                            
                            # Add merge fields first (before any deduplication)
                            if not df_fallback.empty and "oz_vendor_code" in df_fallback.columns:
                                df_fallback = add_merge_fields(df_fallback, wb_sku_col="wb_sku", oz_vendor_col="oz_vendor_code")
                            
                            # Handle duplicates based on filter setting
                            if st.session_state.get("oz_merge_filter_unique_sizes", True) and not df_fallback.empty:
                                # Remove duplicates completely
                                from dataforge.matching_helpers import dedupe_sizes
                                df_fallback = dedupe_sizes(df_fallback, input_type="wb_sku")
                            elif not df_fallback.empty:
                                # Keep duplicates but mark them with 'D' prefix
                                from dataforge.matching_helpers import mark_duplicate_sizes
                                df_fallback = mark_duplicate_sizes(
                                    df_fallback,
                                    primary_prefix="B",
                                    duplicate_prefix="D",
                                    grouping_column="wb_sku"
                                )
                            
                            if not df_fallback.empty and "group_number" in df_fallback.columns:
                                if "group_number" in df_similarity.columns and not df_similarity.empty:
                                    max_existing = df_similarity["group_number"].max()
                                    max_existing_group = int(max_existing) if pd.notna(max_existing) else 0
                                else:
                                    max_existing_group = 0
                                df_fallback["group_number"] = df_fallback["group_number"] + max_existing_group
                            
                            # Add similarity_score column (0 for fallback items)
                            if "similarity_score" not in df_fallback.columns:
                                df_fallback["similarity_score"] = 0.0
                            
                            # Merge with main results
                            st.session_state["oz_merge_result"] = pd.concat([df_similarity, df_fallback], ignore_index=True)
                            st.success(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(df_fallback)} —Å—Ç—Ä–æ–∫ –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞")
                            
                            # Note: No need for final dedupe/mark after merging for similarity algorithm
                            # Both similarity and fallback items already have proper duplicate marking
                except Exception as e:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ fallback –∞–ª–≥–æ—Ä–∏—Ç–º–∞: {e}")
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ similarity, –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞—è —Ä–∞–±–æ—Ç—É
                
                # Update missing list after fallback
                df_final = st.session_state["oz_merge_result"]
                found_wb_final = {str(x) for x in df_final.get("wb_sku", [])} if not df_final.empty else set()
                missing_final = [v for v in values if v not in found_wb_final]
                st.session_state["oz_merge_missing_wb"] = missing_final
                
                if missing_final:
                    st.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤ wb_products –¥–∞–∂–µ –ø–æ—Å–ª–µ –±–∞–∑–æ–≤–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞: {', '.join(missing_final[:50])}{' ...' if len(missing_final)>50 else ''}")
        else:
            # Batch processing with progress
            batch_size = int(st.session_state.get("oz_merge_batch_size", 1000))
            limit_per_input = int(st.session_state.get("oz_merge_limit", 20))
            total = len(values)
            n_chunks = math.ceil(total / batch_size)

            st.session_state["oz_merge_result"] = pd.DataFrame()
            progress = st.progress(0)
            fetched = 0

            for idx in range(n_chunks):
                start = idx * batch_size
                end = min(start + batch_size, total)
                chunk = values[start:end]
                with st.spinner(f"–ó–∞–ø—Ä–æ—Å –ø–∞—Ä—Ç–∏–∏ {idx+1}/{n_chunks} ({start+1}-{end})..."):
                    df_chunk = search_matches(
                        chunk,
                        input_type="wb_sku",
                        limit_per_input=(None if limit_per_input <= 0 else limit_per_input),
                        md_token=md_token,
                        md_database=md_database,
                    )

                if df_chunk is None or df_chunk.empty:
                    df_chunk = pd.DataFrame()

                # Apply defect filter per-chunk to reduce memory
                if st.session_state.get("oz_merge_filter_no_defect", True) and "oz_vendor_code" in df_chunk.columns:
                    starts_with_brak = df_chunk["oz_vendor_code"].fillna("").astype(str).str.startswith(DEFECT_PREFIX)
                    df_chunk = df_chunk.loc[~starts_with_brak]

                # Add merge fields to the chunk
                if not df_chunk.empty and "oz_vendor_code" in df_chunk.columns:
                    df_chunk = add_merge_fields(df_chunk, wb_sku_col="wb_sku", oz_vendor_col="oz_vendor_code")

                # accumulate safely
                df_result = st.session_state.get("oz_merge_result")
                if df_result is None or (hasattr(df_result, "empty") and df_result.empty):
                    st.session_state["oz_merge_result"] = df_chunk.copy()
                else:
                    st.session_state["oz_merge_result"] = pd.concat([df_result, df_chunk], ignore_index=True, copy=False)

                fetched += len(chunk)
                # Update progress
                progress.progress(int((end / total) * 100))

                        # Final dedupe sizes similar to existing page
            filter_unique = st.session_state.get("oz_merge_filter_unique_sizes", True)
            
            if filter_unique and not st.session_state["oz_merge_result"].empty:
                from dataforge.matching_helpers import dedupe_sizes

                st.session_state["oz_merge_result"] = dedupe_sizes(st.session_state["oz_merge_result"], input_type="wb_sku")
                
                # Ensure merge fields present after dedupe
                if not st.session_state["oz_merge_result"].empty:
                    st.session_state["oz_merge_result"] = add_merge_fields(st.session_state["oz_merge_result"], wb_sku_col="wb_sku", oz_vendor_col="oz_vendor_code")
            elif not st.session_state["oz_merge_result"].empty:
                # Keep duplicates but mark them with 'D' prefix (primary uses 'B' for basic algorithm)
                from dataforge.matching_helpers import mark_duplicate_sizes
                
                df_result = st.session_state["oz_merge_result"]
                
                # IMPORTANT: Recalculate group_number based on merge_code after merging all chunks
                # because each chunk had its own group numbering
                unique_merge_codes = sorted(df_result["merge_code"].unique())
                merge_code_to_group = {code: idx + 1 for idx, code in enumerate(unique_merge_codes)}
                df_result["group_number"] = df_result["merge_code"].map(merge_code_to_group)
                
                # For basic algorithm, use wb_sku as grouping column to find duplicates within same wb_sku
                df_result = mark_duplicate_sizes(
                    df_result,
                    primary_prefix="B",
                    duplicate_prefix="D",
                    grouping_column="wb_sku"  # Group by wb_sku instead of group_number
                )
                
                st.session_state["oz_merge_result"] = df_result
                st.session_state["oz_merge_result"] = df_result
                # Note: merge_code is already set by mark_duplicate_sizes, don't call add_merge_fields

            # collect invalid oz_vendor_code rows for highlighting
            df = st.session_state["oz_merge_result"]
            if "oz_vendor_code" in df.columns:
                invalid_mask = ~df["oz_vendor_code"].astype(str).str.contains("-", regex=False)
                st.session_state["oz_merge_invalid_oz_vendor"] = df.loc[invalid_mask]
            else:
                st.session_state["oz_merge_invalid_oz_vendor"] = pd.DataFrame()
    except Exception as exc:
        st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ/–æ–±—Ä–∞–±–æ—Ç–∫–µ: ")
        st.exception(exc)

df_show: pd.DataFrame = st.session_state.get("oz_merge_result", pd.DataFrame())

if df_show.empty:
    st.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –ø–æ–∫–∞–∑–∞–Ω—ã –∑–¥–µ—Å—å –ø–æ—Å–ª–µ –Ω–∞–∂–∞—Ç–∏—è '–°–∫–ª–µ–∏—Ç—å'.")
    st.stop()

cols_default = [
    "group_number",
    "wb_sku",
    "oz_sku",
    "oz_vendor_code",
    "merge_code",
    "merge_color",
    "match_score",
    "similarity_score",
]

cols_to_show = [c for c in st.session_state.get("oz_merge_selected_cols", cols_default) if c in df_show.columns]
if not cols_to_show:
    cols_to_show = [c for c in cols_default if c in df_show.columns]

st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–ª–µ–π–∫–∏ OZ")
st.dataframe(df_show[cols_to_show], width="stretch", height=600)

csv_buf = io.StringIO()
df_show[cols_to_show].to_csv(csv_buf, index=False)
st.download_button(
    "–°–∫–∞—á–∞—Ç—å CSV",
    data=csv_buf.getvalue().encode("utf-8"),
    file_name="oz_merge.csv",
    mime="text/csv",
)
